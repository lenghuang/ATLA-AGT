{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock Paper Scissors Regret Matching\n",
    "\n",
    "Worked example from: http://modelai.gettysburg.edu/2013/cfr/cfr.pdf\n",
    "\n",
    "The high level algorithm is as follows:\n",
    "\n",
    "- For each player, init all cumulative regrets to 0\n",
    "- For some number of iterations:\n",
    "  - Compute a regret-matching strategy profile (if all nonpositive, uniformly random)\n",
    "  - Add strategy profile to strategy profile sum \n",
    "  - Select each player action profile according to strategy profile\n",
    "  - Compute player regrets\n",
    "  - Add player regrets to player cumulative regrets\n",
    "- Return average strategy profile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "First some definitions: \n",
    "\n",
    "- Arbitrarily assign the values 0, 1, 2 to Rock, Paper, Scissors. \n",
    "- Create a RNG\n",
    "- Allocate arrays to hold:\n",
    "  - Accumulated action regrets\n",
    "  - A strategy generated through regret-matching\n",
    "  - Sum of all such strategies generated\n",
    "- Number of iterations we want to run this algo for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "ROCK, PAPER, SCISSORS, NUM_ACTIONS = 0, 1, 2, 3\n",
    "\n",
    "regretSum = [0.0] * NUM_ACTIONS \n",
    "strategy = [0.0] * NUM_ACTIONS\n",
    "strategySum = [0.0] * NUM_ACTIONS\n",
    "oppStrategy = [0.4, 0.3, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Current Mixed Strategy Though Regret-Matching\n",
    "\n",
    "We want to select actions in proportion to positive regrets of not having chosen them.\n",
    "\n",
    "- Copy all positive regrets and sum them \n",
    "- Second pass through strategy entries\n",
    "  - If there is at least one action with positive regret, normalize regrets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES: True\n",
    "# ENSURES:  Get's a strategy lol?\n",
    "\n",
    "def getStrategy():\n",
    "    normalizingSum = 0\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        strategy[a] = regretSum[a] if regretSum[a] > 0 else 0\n",
    "        normalizingSum += strategy[a]\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        if (normalizingSum > 0):\n",
    "            strategy[a] /= normalizingSum\n",
    "        else:\n",
    "            strategy[a] = 1.0 / NUM_ACTIONS # if all nonpositive, make uniform\n",
    "        strategySum[a] += strategy[a]\n",
    "    return strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Random Action According to Mixed-Strategy Distribution\n",
    "\n",
    "Now that we have a mixed strategy, we want to randomly pick an action. \n",
    "\n",
    "- Randomly generate a probability from (0,1]\n",
    "- Check all actions and see if it's less than random probability \n",
    "    - If so, we select that action\n",
    "    - Otherwise return the last action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES: strategy is a float list\n",
    "# ENSURES:  a is a randomly picked action according \n",
    "#           to our mixed strategy distribution\n",
    "\n",
    "def getAction(strategy): \n",
    "    # COME BACK TO THIS ?????\n",
    "    r = random() # we do 1 - since python random gives [0,1)\n",
    "    a = 0\n",
    "    cumulativeProb = 0.0\n",
    "    while(a < NUM_ACTIONS - 1):\n",
    "        cumulativeProb += strategy[a]\n",
    "        if(r < cumulativeProb):\n",
    "            break\n",
    "        a += 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have three parts to our traning algorithm\n",
    "\n",
    "1. Get Regret-matched mixed-strategy actions\n",
    "2. Compute action utilities \n",
    "3. Accumulate action regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES: iterations the int number of times u wanna run this\n",
    "# ENSURES:  void, updates the things accordingly ??\n",
    "\n",
    "def train(iterations):\n",
    "    \n",
    "    actionUtility = [0.0] * NUM_ACTIONS \n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # 1) Get Regret-matched mixed-strategy actions\n",
    "        strategy = getStrategy()\n",
    "        myAction = getAction(strategy)\n",
    "        otherAction = getAction(oppStrategy)\n",
    "\n",
    "        # 2) Compute action utilities\n",
    "        # This is some big brain stuff with modding. \n",
    "        # May hardcode this later for elements\n",
    "        actionUtility[otherAction] = 0\n",
    "        actionUtility[0 if otherAction == NUM_ACTIONS - 1 else otherAction + 1] = 1\n",
    "        actionUtility[NUM_ACTIONS - 1 if otherAction == 0 else otherAction - 1] = -1\n",
    "\n",
    "        # 3) Accumulate action regrets\n",
    "        for a in range(NUM_ACTIONS):\n",
    "            regretSum[a] += actionUtility[a] - actionUtility[myAction]\n",
    "            \n",
    "        if(i % 10000 == 0):\n",
    "            print(\"Iteration \" + str(i))\n",
    "            print(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Average Mixed Strategy Across All Training Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES: strategy is trained\n",
    "# ENSURES:  returns a float list representing average strategy\n",
    "\n",
    "def getAverageStrategy():\n",
    "    \n",
    "    avgStrategy = [0.0] * NUM_ACTIONS\n",
    "    normalizingSum = 0\n",
    "    \n",
    "    for a in range(NUM_ACTIONS):\n",
    "        normalizingSum += strategySum[a]\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        if (normalizingSum > 0):\n",
    "            avgStrategy[a] = strategySum[a] / normalizingSum\n",
    "        else:\n",
    "            avgStrategy[a] = 1.0 / NUM_ACTIONS\n",
    "            \n",
    "    return avgStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Method Initializing Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train(100000)\n",
    "    print(\"Iterations Averaged Out\")\n",
    "    print(getAverageStrategy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
      "Iteration 10000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 20000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 30000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 40000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 50000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 60000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 70000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 80000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iteration 90000\n",
      "[0.0, 1.0, 0.0]\n",
      "Iterations Averaged Out\n",
      "[0.0029810198412698412, 0.9950299801587302, 0.001989]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
